{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6496cb41",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a5f3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c8b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dropped_features.txt', 'w') as f:\n",
    "    f.write(\"=== Dropped Features Log ===\\n\\n\")\n",
    "\n",
    "# Load training data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "X_train = train_data.iloc[:, :-1]  # All columns except target\n",
    "Y_train = train_data.iloc[:, -1]   # Target column\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_ids = test_data['Id']             # Save IDs for later\n",
    "X_test = test_data.drop('Id', axis=1)  # Remove ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c04fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (8250, 40)\n",
      "Test data shape: (5500, 40)\n",
      "Target variable shape: (8250,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the data\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Target variable shape: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd94915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8250 samples, 40 features\n",
      "Test set: 5500 samples, 40 features\n",
      "No missing values in training data.\n",
      "No missing values in test data.\n",
      "Number of duplicate rows in training data: 0\n",
      "Number of duplicate rows in test data: 0\n",
      "No constant features in training data.\n",
      "No constant features in test data.\n"
     ]
    }
   ],
   "source": [
    "# Check size\n",
    "n_samples, n_features = X_train.shape\n",
    "print(f\"Training set: {n_samples} samples, {n_features} features\")\n",
    "\n",
    "n_samples_test, n_features_test = X_test.shape\n",
    "print(f\"Test set: {n_samples_test} samples, {n_features_test} features\")\n",
    "\n",
    "# Check missing values\n",
    "missing_values = X_train.isnull().sum()\n",
    "if missing_values[missing_values > 0].empty:\n",
    "    print(\"No missing values in training data.\")\n",
    "else:\n",
    "    print(\"Missing values in training data:\\n\", missing_values[missing_values > 0])\n",
    "\n",
    "missing_values_test = X_test.isnull().sum()\n",
    "if missing_values_test[missing_values_test > 0].empty:\n",
    "    print(\"No missing values in test data.\")\n",
    "else:\n",
    "    print(\"Missing values in test data:\\n\", missing_values_test[missing_values_test > 0])\n",
    "\n",
    "# Check duplicate rows\n",
    "duplicate_rows = X_train.duplicated().sum()\n",
    "print(f\"Number of duplicate rows in training data: {duplicate_rows}\")\n",
    "\n",
    "duplicate_rows_test = X_test.duplicated().sum()\n",
    "print(f\"Number of duplicate rows in test data: {duplicate_rows_test}\")\n",
    "\n",
    "# Check constant features\n",
    "constant_features = X_train.columns[X_train.nunique() <= 1]\n",
    "if constant_features.empty:\n",
    "    print(\"No constant features in training data.\")\n",
    "else:\n",
    "    print(\"Constant features in training data:\\n\", constant_features.tolist())\n",
    "\n",
    "constant_features_test = X_test.columns[X_test.nunique() <= 1]\n",
    "if constant_features_test.empty:\n",
    "    print(\"No constant features in test data.\")\n",
    "else:\n",
    "    print(\"Constant features in test data:\\n\", constant_features_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24778666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature correlations with target variable:\n",
      "[0.07964190054974579, -0.06128885266004798, 0.33442013907309565, 0.3969744453356405, 0.3080799621532907, 0.10601459861929727, -0.7045147870681104, -0.056970294145277905, -0.052774188141616725, -0.07356026312089133, 0.6413120025165503, 0.6404046428202611, 0.6404046428202611, 0.6403982947247879, 0.6404133411853214, 0.6388169578630332, 0.6388288800726144, 0.6363738722472071, 0.6363738722472071, 0.6335668974934874, 0.6337070768152012, 0.630812268208842, 0.630761169874003, 0.6283347894625324, 0.03530695033232792, -0.009046901236544908, 0.016616528910079087, -0.019433185595649934, 0.010640925795167937, -0.019433185595649934, 0.03211506117122081, -0.017202457805639306, 0.0402020337875995, -0.0028748441628541066, 0.027329701051732613, -0.008917829140455918, 0.025826204301336295, -0.024296226752424314, 0.6159202211674143, 0.6282719851321914]\n",
      "Feature p-values with target variable:\n",
      "[4.348468950928982e-13, 2.5294625218109232e-08, 1.0073647425945063e-214, 1.60850058891493e-309, 6.998045322196341e-181, 4.680990898650704e-22, 0.0, 2.242708212407471e-07, 1.6180915772596997e-06, 2.239434034137988e-11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0013392802557110263, 0.4112943918598027, 0.13126130330620658, 0.07756276799424848, 0.33384857159040826, 0.07756276799424848, 0.003530695072434924, 0.1182009294238787, 0.0002597458906693141, 0.7940295334616118, 0.013048938144436294, 0.41800028245981086, 0.018985411529488266, 0.02732763463855904, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Feature selection based on training data only\n",
    "p_values = []\n",
    "correlations = []\n",
    "for col in X_train.columns:\n",
    "    corr, p_val = stats.pearsonr(X_train[col], Y_train)\n",
    "    correlations.append(corr)\n",
    "    p_values.append(p_val)\n",
    "print(\"Feature correlations with target variable:\")\n",
    "print(correlations)\n",
    "print(\"Feature p-values with target variable:\")\n",
    "print(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833a3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the correlation matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.dates as mdates\n",
    "\n",
    "# Xy_train = X_train.copy()\n",
    "# Xy_train['target'] = Y_train\n",
    "\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# sns.heatmap(Xy_train.corr(), annot=False, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('correlation_matrix.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cef74503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Correlation Analysis:\n",
      "             Feature  Correlation        P-value\n",
      "6     absoluate_roll    -0.704515   0.000000e+00\n",
      "10             time1     0.641312   0.000000e+00\n",
      "14             time5     0.640413   0.000000e+00\n",
      "11             time2     0.640405   0.000000e+00\n",
      "12             time3     0.640405   0.000000e+00\n",
      "13             time4     0.640398   0.000000e+00\n",
      "16             time7     0.638829   0.000000e+00\n",
      "15             time6     0.638817   0.000000e+00\n",
      "18             time9     0.636374   0.000000e+00\n",
      "17             time8     0.636374   0.000000e+00\n",
      "20            time11     0.633707   0.000000e+00\n",
      "19            time10     0.633567   0.000000e+00\n",
      "21            time12     0.630812   0.000000e+00\n",
      "22            time13     0.630761   0.000000e+00\n",
      "23            time14     0.628335   0.000000e+00\n",
      "39               set     0.628272   0.000000e+00\n",
      "38             omega     0.615920   0.000000e+00\n",
      "3                  n     0.396974  1.608501e-309\n",
      "2                  m     0.334420  1.007365e-214\n",
      "4      current_pitch     0.308080  6.998045e-181\n",
      "5       current_roll     0.106015   4.680991e-22\n",
      "0           acc_rate     0.079642   4.348469e-13\n",
      "9   climb_delta_diff    -0.073560   2.239434e-11\n",
      "1              track    -0.061289   2.529463e-08\n",
      "7        climb_delta    -0.056970   2.242708e-07\n",
      "8    roll_rate_delta    -0.052774   1.618092e-06\n",
      "32       time9_delta     0.040202   2.597459e-04\n",
      "24       time1_delta     0.035307   1.339280e-03\n",
      "30       time7_delta     0.032115   3.530695e-03\n",
      "34      time11_delta     0.027330   1.304894e-02\n",
      "36      time13_delta     0.025826   1.898541e-02\n",
      "37      time14_delta    -0.024296   2.732763e-02\n",
      "27       time4_delta    -0.019433   7.756277e-02\n",
      "29       time6_delta    -0.019433   7.756277e-02\n",
      "31       time8_delta    -0.017202   1.182009e-01\n",
      "26       time3_delta     0.016617   1.312613e-01\n",
      "28       time5_delta     0.010641   3.338486e-01\n",
      "25       time2_delta    -0.009047   4.112944e-01\n",
      "35      time12_delta    -0.008918   4.180003e-01\n",
      "33      time10_delta    -0.002875   7.940295e-01\n"
     ]
    }
   ],
   "source": [
    "feature_stats = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Correlation': correlations,\n",
    "    'P-value': p_values\n",
    "})\n",
    "\n",
    "feature_stats['Abs_Correlation'] = abs(feature_stats['Correlation'])\n",
    "feature_stats = feature_stats.sort_values('Abs_Correlation', ascending=False)\n",
    "feature_stats = feature_stats.drop('Abs_Correlation', axis=1)\n",
    "\n",
    "with open('dropped_features.txt', 'a') as f:\n",
    "    f.write(\"Feature Correlation Analysis:\\n\")\n",
    "    f.write(feature_stats.to_string())\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "print(\"Feature Correlation Analysis:\")\n",
    "print(feature_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03c8a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop features with p-value > 0.05\n",
    "cols_to_drop_1 = X_train.columns[np.array(p_values) > 0.05] # Drop features with p-value > 0.05\n",
    "with open('dropped_features.txt', 'a') as f:\n",
    "    f.write(\"Step 1 - Dropped features (p-value > 0.05):\\n\")\n",
    "    f.write(\", \".join(cols_to_drop_1) + \"\\n\\n\")\n",
    "\n",
    "X_train_filtered = X_train.drop(columns=cols_to_drop_1)\n",
    "feature_names = X_train_filtered.columns.tolist()\n",
    "\n",
    "# Step 2: Analyze feature correlations\n",
    "p_value_matrix = np.zeros((X_train_filtered.shape[1], X_train_filtered.shape[1]))\n",
    "for i in range(X_train_filtered.shape[1]):\n",
    "    for j in range(X_train_filtered.shape[1]):\n",
    "        _, p_val = stats.pearsonr(X_train_filtered.iloc[:, i], X_train_filtered.iloc[:, j])\n",
    "        p_value_matrix[i, j] = p_val\n",
    "\n",
    "p_value_counts = np.sum(p_value_matrix == 0, axis=1)\n",
    "\n",
    "with open('dropped_features.txt', 'a') as f:\n",
    "    f.write(\"Step 2 - Count of zero p-values between features:\\n\")\n",
    "    for name, count in zip(feature_names, p_value_counts):\n",
    "        f.write(f\"{name}: {count}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "cols_to_drop_indices = np.where(p_value_counts >= 10)[0]\n",
    "cols_to_drop_2 = [\n",
    "    feature_names[i] \n",
    "    for i in cols_to_drop_indices \n",
    "    if feature_names[i] != 'time1'\n",
    "]\n",
    "\n",
    "with open('dropped_features.txt', 'a') as f:\n",
    "    f.write(\"Step 2 - Dropped features:\\n\")\n",
    "    f.write(\", \".join(cols_to_drop_2) + \"\\n\\n\")\n",
    "\n",
    "# Apply the same feature dropping to both train and test\n",
    "X_train_final = X_train_filtered.drop(columns=cols_to_drop_2)\n",
    "X_test_final = X_test.drop(columns=list(cols_to_drop_1) + cols_to_drop_2)\n",
    "\n",
    "# Save the list of final features for future reference\n",
    "final_features = X_train_final.columns.tolist()\n",
    "with open('final_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a559eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "standard_scaler_features = ['acc_rate', 'track', 'current_roll', 'climb_delta', 'roll_rate_delta', 'climb_delta_diff']\n",
    "robust_scaler_features = ['m', 'absoluate_roll', 'time1']\n",
    "minmax_scaler_features = ['time1_delta', 'time7_delta', 'time9_delta', 'time11_delta', 'time13_delta', 'time14_delta']\n",
    "\n",
    "# Filter scaling feature lists to only include columns that remain after feature selection\n",
    "standard_scaler_features = [f for f in standard_scaler_features if f in X_train_final.columns]\n",
    "robust_scaler_features = [f for f in robust_scaler_features if f in X_train_final.columns]\n",
    "minmax_scaler_features = [f for f in minmax_scaler_features if f in X_train_final.columns]\n",
    "\n",
    "# Create column transformer for feature scaling\n",
    "column_trans = ColumnTransformer([\n",
    "    (\"standard\", StandardScaler(), standard_scaler_features),\n",
    "    (\"robust\", RobustScaler(), robust_scaler_features),\n",
    "    (\"minmax\", MinMaxScaler(feature_range=(-1,1)), minmax_scaler_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Fit the transformer on training data only\n",
    "column_trans.fit(X_train_final)\n",
    "\n",
    "# Save the transformer for future use\n",
    "with open('feature_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(column_trans, f)\n",
    "\n",
    "# Transform both train and test data\n",
    "X_train_scaled = column_trans.transform(X_train_final)\n",
    "X_test_scaled = column_trans.transform(X_test_final)\n",
    "\n",
    "# Get the column names in the correct order after transformation\n",
    "transformed_columns = (\n",
    "    standard_scaler_features + \n",
    "    robust_scaler_features + \n",
    "    minmax_scaler_features + \n",
    "    [col for col in X_train_final.columns if col not in \n",
    "     standard_scaler_features + robust_scaler_features + minmax_scaler_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "823bbd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8250 training samples with 15 features\n",
      "Processed 5500 test samples with 15 features\n",
      "Feature selection and scaling complete. Files saved.\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "pd.DataFrame(X_train_scaled, columns=transformed_columns).to_csv('X_train_processed.csv', index=False)\n",
    "Y_train.to_csv('Y_train_processed.csv', index=False)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=transformed_columns)\n",
    "X_test_df.to_csv('X_test_processed.csv', index=False)\n",
    "\n",
    "# Save full preprocessed datasets with proper IDs for training and submission\n",
    "train_processed = pd.DataFrame(X_train_scaled, columns=transformed_columns)\n",
    "train_processed['target'] = Y_train.values\n",
    "train_processed.to_csv('train_processed.csv', index=False)\n",
    "\n",
    "test_processed = pd.DataFrame(X_test_scaled, columns=transformed_columns)\n",
    "test_processed.insert(0, 'Id', test_ids)  # Add Id column back at the beginning\n",
    "test_processed.to_csv('test_processed.csv', index=False)\n",
    "\n",
    "print(f\"Processed {X_train_scaled.shape[0]} training samples with {X_train_scaled.shape[1]} features\")\n",
    "print(f\"Processed {X_test_scaled.shape[0]} test samples with {X_test_scaled.shape[1]} features\")\n",
    "print(\"Feature selection and scaling complete. Files saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbcef80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training data shape: (8250, 15)\n",
      "Final test data shape: (5500, 15)\n",
      "Final features: ['acc_rate', 'track', 'current_roll', 'climb_delta', 'roll_rate_delta', 'climb_delta_diff', 'm', 'absoluate_roll', 'time1', 'time1_delta', 'time7_delta', 'time9_delta', 'time11_delta', 'time13_delta', 'time14_delta']\n",
      "Number of features after selection: 15\n",
      "Number of features dropped: 25\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the final datasets\n",
    "print(f\"Final training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Final test data shape: {X_test_scaled.shape}\")\n",
    "print(f\"Final features: {transformed_columns}\")\n",
    "print(f\"Number of features after selection: {len(transformed_columns)}\")\n",
    "print(f\"Number of features dropped: {len(X_train.columns) - len(transformed_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6b74c",
   "metadata": {},
   "source": [
    "## Training Model and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f997a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training data shape: (8250, 15)\n",
      "Test data shape: (5500, 15)\n"
     ]
    }
   ],
   "source": [
    "# Create best_model folder if not exists\n",
    "os.makedirs('best_model', exist_ok=True)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "train_processed = pd.read_csv('train_processed.csv')\n",
    "X = train_processed.iloc[:, :-1].values\n",
    "Y = train_processed.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "test_processed = pd.read_csv('test_processed.csv')\n",
    "test_ids = test_processed['Id'].values\n",
    "X_test = test_processed.drop('Id', axis=1).values\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Define model with Dropout and Batch Normalization\n",
    "class RegressionANN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "# Random search space\n",
    "def sample_hyperparams():\n",
    "    return {\n",
    "        'learning_rate': 10**random.uniform(-4, -2),\n",
    "        'batch_size': random.choice([8, 16, 32, 64]),\n",
    "        'weight_decay': 10**random.uniform(-5, -3)\n",
    "    }\n",
    "\n",
    "# Hyperparameters\n",
    "total_epochs = 500\n",
    "patience = 30\n",
    "n_random_search = 10  # Number of random trials\n",
    "\n",
    "best_global_rmse = np.inf\n",
    "best_hyperparams = None\n",
    "random_search_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(n_random_search):\n",
    "    print(f\"\\nRandom Search Trial {trial+1}/{n_random_search}\")\n",
    "    params = sample_hyperparams()\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    weight_decay = params['weight_decay']\n",
    "\n",
    "    print(f\"Trying hyperparams: lr={learning_rate:.5f}, batch_size={batch_size}, weight_decay={weight_decay:.5f}\")\n",
    "\n",
    "    # KFold Cross Validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold = 1\n",
    "    val_rmse_list = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32))\n",
    "        val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.float32))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = RegressionANN(X.shape[1]).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        best_val_rmse = np.inf\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(total_epochs):\n",
    "            model.train()\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_preds = []\n",
    "            val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    val_preds.append(outputs.cpu().numpy())\n",
    "                    val_targets.append(targets.cpu().numpy())\n",
    "\n",
    "            val_preds = np.vstack(val_preds)\n",
    "            val_targets = np.vstack(val_targets)\n",
    "\n",
    "            val_rmse = root_mean_squared_error(val_targets, val_preds)\n",
    "\n",
    "            if val_rmse < best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "        val_rmse_list.append(best_val_rmse)\n",
    "        fold += 1\n",
    "\n",
    "    avg_val_rmse = np.mean(val_rmse_list)\n",
    "    random_search_results.append((params, avg_val_rmse))\n",
    "    print(f\"Avg Validation RMSE for this setting: {avg_val_rmse:.5f}\")\n",
    "\n",
    "    if avg_val_rmse < best_global_rmse:\n",
    "        best_global_rmse = avg_val_rmse\n",
    "        best_hyperparams = params\n",
    "\n",
    "print(f\"\\nBest Hyperparameters Found: {best_hyperparams}\")\n",
    "print(f\"Best Validation RMSE: {best_global_rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd83370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot Hyperparameters vs Validation RMSE\n",
    "# lrs = [np.log10(r[0]['learning_rate']) for r in random_search_results]\n",
    "# weight_decays = [np.log10(r[0]['weight_decay']) for r in random_search_results]\n",
    "# batch_sizes = [r[0]['batch_size'] for r in random_search_results]\n",
    "# val_rmses = [r[1] for r in random_search_results]\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(lrs, val_rmses)\n",
    "# plt.xlabel('log10(Learning Rate)')\n",
    "# plt.ylabel('Validation RMSE')\n",
    "# plt.title('Learning Rate vs Validation RMSE')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(weight_decays, val_rmses)\n",
    "# plt.xlabel('log10(Weight Decay)')\n",
    "# plt.ylabel('Validation RMSE')\n",
    "# plt.title('Weight Decay vs Validation RMSE')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(batch_sizes, val_rmses)\n",
    "# plt.xlabel('Batch Size')\n",
    "# plt.ylabel('Validation RMSE')\n",
    "# plt.title('Batch Size vs Validation RMSE')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义 best_hyperparams\n",
    "# best_hyperparams = {\n",
    "#     'learning_rate': 0.0050705147268348535,\n",
    "#     'batch_size': 64,\n",
    "#     'weight_decay': 2.0472230821749646e-05\n",
    "# }\n",
    "\n",
    "# # 转为 DataFrame 并保存为 CSV\n",
    "# df = pd.DataFrame([best_hyperparams])\n",
    "# df.to_csv('best_hyperparams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e41e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 读取 CSV\n",
    "# df = pd.read_csv('best_hyperparams.csv')\n",
    "\n",
    "# # 提取参数\n",
    "# best_hyperparams = df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d1db4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_hyperparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Retrain with best hyperparameters\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m best_hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m best_hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m best_hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_hyperparams' is not defined"
     ]
    }
   ],
   "source": [
    "# Retrain with best hyperparameters\n",
    "learning_rate = best_hyperparams['learning_rate']\n",
    "batch_size = best_hyperparams['batch_size']\n",
    "weight_decay = best_hyperparams['weight_decay']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_test_preds = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = RegressionANN(X.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_rmse = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            val_targets = torch.tensor(Y_val, dtype=torch.float32).to(device)\n",
    "            outputs = model(val_inputs)\n",
    "            val_preds = outputs.cpu().numpy()\n",
    "            val_targets = val_targets.cpu().numpy()\n",
    "\n",
    "        val_rmse = root_mean_squared_error(val_targets, val_preds)\n",
    "\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model/final_best_model_fold{fold}.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(f'best_model/final_best_model_fold{fold}.pt'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "        test_pred = model(X_test_tensor).cpu().numpy()\n",
    "        all_test_preds.append(test_pred)\n",
    "\n",
    "# Ensemble prediction\n",
    "test_pred_final = np.mean(all_test_preds, axis=0)\n",
    "\n",
    "# Save submission\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_ids,\n",
    "    'target': test_pred_final.flatten()\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\nFinal predictions saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfaa4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
